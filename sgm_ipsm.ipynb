{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score-based Generative Modeling with implicit potential score matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import lib.toy_data as toy_data\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import deepxde as dde\n",
    "from deepxde.backend import pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing for scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('simple_sgm_experiments')\n",
    "parser.add_argument('--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'], type = str,default = 'moons')\n",
    "parser.add_argument('--depth',help = 'number of hidden layers of score network',type =int, default = 7)\n",
    "parser.add_argument('--hiddenunits',help = 'number of nodes per hidden layer', type = int, default = 32)\n",
    "parser.add_argument('--niters',type = int, default = 100000)\n",
    "parser.add_argument('--batch_size', type = int,default = 64)\n",
    "parser.add_argument('--lr',type = float, default = 1e-3) \n",
    "parser.add_argument('--finalT',type = float, default = 5)\n",
    "parser.add_argument('--dt',type = float,help = 'integrator step size', default = 0.001)\n",
    "parser.add_argument('--save',type = str,default = 'experiments/simple_sgm/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change parameters here (comment out below for default options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameters\n",
    "args = parser.parse_args()\n",
    "\n",
    "learning_rate = args.lr # learning rate for training neural network\n",
    "batch_size = args.batch_size # batch size during training of neural network\n",
    "epochs = args.niters # Number of training epochs for the neural network\n",
    "T = args.finalT   # Forward simulation time in the forward SDE\n",
    "dataset = args.data # Dataset choice, see toy_data for full options of toy datasets ('checkerboard','8gaussians','2spirals','swissroll','moons',etc.)\n",
    "depth = args.depth\n",
    "# args.dt = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score network functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_score_model(depth:int, hidden_units:int):\n",
    "    '''\n",
    "    Initializes neural network that models the score function\n",
    "    '''\n",
    "    chain = []\n",
    "    chain.append(nn.Linear(3,int(hidden_units),bias =True)) # hard coded input side: (t, x) ∊ R x R^2\n",
    "    chain.append(nn.GELU())\n",
    "\n",
    "    for _ in range(depth-1):\n",
    "        chain.append(nn.Linear(int(hidden_units),int(hidden_units),bias = True))\n",
    "        chain.append(nn.GELU())\n",
    "    chain.append(nn.Linear(int(hidden_units),2,bias = True)) # hard coded output size: x ∊ R^2\n",
    "\n",
    "    return nn.Sequential(*chain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score matching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_samples(t:torch.Tensor, samples:torch.Tensor):\n",
    "    '''\n",
    "    Noises each entry of `samples` tensor forward by the corresponding entry in the time tensor `t`\n",
    "    '''\n",
    "    sigmas = torch.sqrt(1 - torch.exp(-t))\n",
    "    # makes a column vec of length (# samples). * arg is to make sure the dimensions dim(samples)\n",
    "    sigmas = sigmas.view(samples.shape[0],*([1]*len(samples.shape[1:])))\n",
    "    # Generate noise ξ for each coordinate of each sample, and multiply by sigma\n",
    "    noise = torch.randn_like(samples) * sigmas\n",
    "    # repeat each element time vector so that it can be added to each coordinate in a sample\n",
    "    # transpose to match dimension of system of samples\n",
    "    t_enlarge = t.repeat(2,1).T\n",
    "    # Finally add the noise to each data point, computing x_i^{t_i} \n",
    "    perturbed_samples = torch.tensor(samples * torch.exp(-0.5 * t_enlarge) + noise,requires_grad=True) \n",
    "\n",
    "    return perturbed_samples\n",
    "\n",
    "def time_ism_score_estimator(scorenet:nn.Sequential,samples:torch.Tensor,Tmin:int,Tmax:int,eps:int):\n",
    "    '''\n",
    "    Given learned score function `scorenet` and samples `samples`, returns value of ISM objective function function.\n",
    "    '''\n",
    "    t = torch.rand(samples.shape[0]) * (Tmax - Tmin - eps) + eps + Tmin # sample uniformly from time interval\n",
    "\n",
    "    perturbed_samples = noise_samples(t, samples) # add noise to samples\n",
    "\n",
    "    # Evaluate the score function\n",
    "    score_eval_samples = torch.cat((t.reshape(-1,1),perturbed_samples),1)\n",
    "    scores = scorenet(score_eval_samples)\n",
    "\n",
    "    # Evaluate the divergence\n",
    "    s1x = dde.gradients.jacobian(scores,score_eval_samples,i = 0,j = 1)\n",
    "    s2y = dde.gradients.jacobian(scores,score_eval_samples,i = 1,j = 2)\n",
    "\n",
    "    s1x = s1x.view(s1x.shape[0],-1)\n",
    "    s1x = s1x.sum(dim=-1)\n",
    "\n",
    "    s2y = s2y.view(s2y.shape[0],-1)\n",
    "    s2y = s2y.sum(dim=-1)\n",
    "\n",
    "    scores = scores.view(scores.shape[0],-1)\n",
    "\n",
    "    # Evaluate object function\n",
    "    loss = ((s1x+s2y) + 0.5*(scores**2).sum(dim = -1)) \n",
    "\n",
    "    return loss.mean(dim = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDE Dynamics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ou_dynamics(init, T):\n",
    "    '''\n",
    "    Solves the OU process exactly given deterministic initial condition `init` and final time `T`\n",
    "    '''\n",
    "    return init * torch.exp(- 0.5 * T) + torch.sqrt(1-torch.exp(-T)) * torch.randn_like(init)\n",
    "\n",
    "def reverse_sde(score, init,T,lr=args.dt):\n",
    "    '''\n",
    "    Given learned score function `score`, reverses random datapoints `init`\n",
    "    '''\n",
    "    step = int(T/lr) \n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(init.shape[0],1),init),1)\n",
    "        init = init + current_lr  * (init/2 + score(evalpoint).detach() )\n",
    "        init = init + torch.randn_like(init) * np.sqrt(current_lr)\n",
    "    return init\n",
    "\n",
    "def reverse_ode_flow(score,init,T,lr = args.dt):\n",
    "    '''\n",
    "    The deterministic ODE flow that can also sample from the target distribution\n",
    "    '''\n",
    "    step = int(T/lr)\n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(init.shape[0],1),init),1)\n",
    "        init = init + current_lr  * (init/2 + 1/2 * score(evalpoint).detach())\n",
    "    return init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(scorenet: nn.Sequential):\n",
    "    '''\n",
    "    Uses learned `scorenet` to generate new samples ~ π from Gaussian noise\n",
    "    '''\n",
    "    noise = torch.randn(10000, 2) \n",
    "    generated_samples = reverse_sde(scorenet, noise, torch.tensor(T)).detach().numpy()\n",
    "    return generated_samples\n",
    "\n",
    "def save_training_slice(samples, epoch, dataset):\n",
    "    plt.clf()\n",
    "    plt.scatter(samples[:,0],samples[:,1],s = 0.1)\n",
    "    plt.axis('square')\n",
    "\n",
    "    plt.savefig(f'images/training_slices/{dataset}/{epoch}.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize score network\n",
    "scorenet = construct_score_model(args.depth,args.hiddenunits)\n",
    "print(scorenet)\n",
    "optimizer = optim.Adam(scorenet.parameters(), lr=args.lr)\n",
    "\n",
    "# Training the score network\n",
    "p_samples = toy_data.inf_train_gen(dataset,batch_size = 1000000)\n",
    "training_samples = torch.tensor(p_samples).to(dtype = torch.float32)\n",
    "for step in range(epochs):\n",
    "    # sample toy_data\n",
    "    randind = torch.randint(0,1000000,[batch_size,])\n",
    "    samples = training_samples[randind,:]\n",
    "\n",
    "    # evaluate loss function and gradient\n",
    "    # t = torch.linspace(0,T,100)\n",
    "    loss = time_ism_score_estimator(scorenet,samples,0,T,eps = 0.001)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update score network\n",
    "    optimizer.step()\n",
    "\n",
    "    if not step%100:\n",
    "        print(loss,step)\n",
    "    \n",
    "    if not step % 1000:\n",
    "        denoise(scorenet)\n",
    "        save_training_slice(samples, step, dataset)\n",
    "\n",
    "# Denoising the normal distribution \n",
    "samples_lang = denoise(scorenet)\n",
    "\n",
    "# # Denoising samples from the training data\n",
    "# samples = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 10000))\n",
    "# samples_lang_noisedtraining = samples * torch.exp(-0.5 * torch.tensor(T)) + torch.sqrt(1-torch.exp(-torch.tensor(T))) * torch.randn_like(samples)\n",
    "# samples_lang_noisedtraining =reverse_sde(scorenet, samples_lang_noisedtraining.to(dtype=torch.float32),torch.tensor(T)).detach().numpy()\n",
    "\n",
    "# # Deterministically evolving the normal distribution \n",
    "# samples_lang_deterministic = torch.randn(10000,2)\n",
    "# samples_lang_deterministic = reverse_ode_flow(scorenet,samples_lang_deterministic,torch.tensor(T)).detach().numpy()\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_lang[:,0],samples_lang[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "# plt.axis([-5 ,5 ,-5, 5])\n",
    "\n",
    "# Plot truth\n",
    "\n",
    "plt.clf()\n",
    "p_samples = toy_data.inf_train_gen(dataset, batch_size = 10000)\n",
    "samples_true = torch.tensor(p_samples).to(dtype = torch.float32)\n",
    "plt.scatter(samples_true[:,0],samples_true[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "# plt.axis([-5 ,5 ,-5, 5])\n",
    "plt.title('Truth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "particlesystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
